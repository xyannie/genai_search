{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encyclopedia P.O.C\n",
    "\n",
    "This notebook is dedicated to exploring the usage of GENAI to develop encyclopedic facts based on search terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIBRARIES\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import re\n",
    "import os\n",
    "import emoji\n",
    "import json\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "\n",
    "ENCYCLOPEDIA_PROMPT = \"\"\"\n",
    "A user searches the following query on the MoneyLion app\n",
    "search_query : {search_query}\n",
    "Instructions:\n",
    "Share a fun, one liner informative financial fact about {search_query} in under 50 words.\n",
    "This fact can be a piece of advice, a fun fact or a statistic.\n",
    "Provide the source of the fact such as the name of the website or the name of the book.\n",
    "Do not produce a fact that can negatively impact MoneyLion's brand or reputation.\n",
    "Do not provide alternatives to MoneyLion's products or services.\n",
    "\n",
    "Output the final answer in the following format:\n",
    "    Generated Fact:(input generated fact here)\n",
    "    Source:(input source here)\n",
    "    Type of fact:(input type of fact here)\n",
    "\"\"\"\n",
    "\n",
    "EVALUATION_PROMPT = \"\"\"\n",
    "Here's is a generated fact about {search_query}:\n",
    "{fact}  \n",
    "\n",
    "Please evaluate the following sentence for the following criterias:\n",
    "-Is the fact generated a financial and related to {search_query}?\n",
    "-Is the source of the fact credible?\n",
    "\n",
    "\n",
    "Output criteria:\n",
    "    - return final answer using 4 variables \"result_rating\", \"result_relevance\", \"source_credibility\" and \"rating_reason\"\n",
    "    - compute \"result_rating\" as: Rate relevance of fact on the scale of 0 to 10, with 0 being not financial fact, and 10 being a useful financial fact\n",
    "    - compute \"result_relevance\" as: If the fact is relevant to {search_query}, return 1. Else return 0\n",
    "    - compute \"source_credibility\" as: If the source of the fact is a credible source, return 1. Else return 0\n",
    "    - compute \"rating_reason\" as: Reasonings of the given score for \"result_rating\", \"result_relevance\" and \"source_credibility\" in detail (in 1-2 sentences)\n",
    "    - output final answer in the following format: \n",
    "        result_rating=(input value of result_rating here); result_relevance=(input value of result_relevance here); source_credibility=(input value of source_credibility here); rating_reason=(input value of rating_reason here)\n",
    "\n",
    "let's think step by step\n",
    "\"\"\"\n",
    "\n",
    "PRODUCT_KEYWORDS = [\n",
    "    # 'Peer to Peer Payments',\n",
    "    'Roar Money',\n",
    "    'Instacash',\n",
    "    # 'Investment Account',\n",
    "    'Credit Builder Plus'\n",
    "]\n",
    "\n",
    "PRODUCT_ACTION_KEYWORDS = [\n",
    "    # 'Direct Deposit',\n",
    "    'Peer Boost',\n",
    "    'Shake n Bank',\n",
    "    # 'Round Ups',\n",
    "    'Cash Advance',\n",
    "]\n",
    "\n",
    "COMPETITOR_KEYWORDS = [\n",
    "    \"SoFi\",\n",
    "    \"Cleo\",\n",
    "    \"Mission Lane\",\n",
    "    \"Propel\",\n",
    "    \"Dave\",\n",
    "    \"Braviant Holdings\",\n",
    "    \"EarnIn\",\n",
    "    \"Brigit\",\n",
    "    \"Affirm\",\n",
    "    \"Avant\",\n",
    "    \"Varo\",\n",
    "    \"Revolut\",\n",
    "    \"Monso\",\n",
    "    \"Acorn\",\n",
    "    \"Betterment\",\n",
    "    \"Chime\",\n",
    "]\n",
    "\n",
    "GENERATED_FACT_PATTERN = r\"Generated Fact: (.*?)\\n\"\n",
    "SOURCE_PATTERN = r\"Source: (.*?)\\n\"\n",
    "TYPE_OF_FACT_PATTERN = r\"Type of fact: (.*?)$\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKING FUNCTION USED FOR BOTH PRE AND POST PROCESSING\n",
    "def check_keywords(query):\n",
    "    \"\"\"\n",
    "    Check if the query contains any of the product keywords.\n",
    "    Used for both pre and post processing steps\n",
    "\n",
    "    params:\n",
    "        query: the query to be checked\n",
    "    \n",
    "    returns:\n",
    "        True if the query contains any of the product keywords\n",
    "        False otherwise\n",
    "    \"\"\"\n",
    "    keyword_list = PRODUCT_KEYWORDS + PRODUCT_ACTION_KEYWORDS + COMPETITOR_KEYWORDS\n",
    "    for keyword in keyword_list:\n",
    "        if keyword.lower() in query.lower() or keyword.lower().replace(' ', '') in query.lower():\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATION FUNCTIONS\n",
    "def get_gpt_response(prompt, temperature=1.0, model = \"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    To generate content using GPT\n",
    "    We use the GPT-3.5-turbo model for generating content\n",
    "    For offline testing however, we use GPT 4 for generating the evaluation score\n",
    "\n",
    "    params:\n",
    "        prompt: the prompt to be used for generating content\n",
    "        temperature: the temperature to be used for generating content/evaluation score\n",
    "        model: the model to be used for generating content/evaluation score\n",
    "\n",
    "    returns:\n",
    "        response: the response from the GPT API\n",
    "    \"\"\"\n",
    "    CHAT_COMPLETION_MODEL = model\n",
    "    CHAT_COMPLETION_API_PARAMS = {\n",
    "        \"temperature\": temperature,\n",
    "        \"model\": CHAT_COMPLETION_MODEL,\n",
    "    }\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        messages=messages, **CHAT_COMPLETION_API_PARAMS\n",
    "    )\n",
    "\n",
    "    response_content = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    return response, response_content\n",
    "\n",
    "\n",
    "def main_generate_fact (query):\n",
    "    \"\"\"\n",
    "    Executes the following steps:\n",
    "        1. Check if the query contains any of the product keywords\n",
    "        2. If no, generate the fact using GPT. If yes, return None\n",
    "        3. Post process the generated fact\n",
    "\n",
    "    params:\n",
    "        query: the query to be checked\n",
    "\n",
    "    returns:\n",
    "        fact: the generated output   \n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        query_flag = check_keywords(query)\n",
    "        if query_flag:\n",
    "            fact = \"Query contains restricted keywords\"\n",
    "            print(\"The matching keyword is: \", query)\n",
    "            return fact, query_flag\n",
    "        else:\n",
    "                prompt = ENCYCLOPEDIA_PROMPT.format(search_query=query)\n",
    "                response, fact = get_gpt_response(prompt)\n",
    "                output_flag = check_keywords(fact)\n",
    "    except Exception as error:\n",
    "        print(\"Error in generating fact\")\n",
    "        print(error)\n",
    "    \n",
    "    return fact, output_flag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST-PROCESSING FUNCTIONS\n",
    "\n",
    "def remove_emojis(text):\n",
    "    \"\"\"\n",
    "    Remove all emojis in title\n",
    "\n",
    "    params:\n",
    "        text: the text to be cleaned\n",
    "    \n",
    "    returns:\n",
    "        cleaned_text: the cleaned text\n",
    "    \"\"\"\n",
    "    cleaned_text = ''\n",
    "    for words in text:\n",
    "        if words not in emoji.EMOJI_DATA:\n",
    "            cleaned_text += words\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_unnecessary_char(text):\n",
    "    \"\"\"\n",
    "    Remove leading and trailing whitespace and newline characters, exclamation marks (!), and quotation marks (\") from the text.\n",
    "\n",
    "    params:\n",
    "        text: the text to be cleaned\n",
    "\n",
    "    returns:\n",
    "        cleaned_text: the cleaned text\n",
    "    \"\"\"\n",
    "    text = text.strip(\" \\n\")\n",
    "    t1 = re.sub(\"\\s*!\\s*\", repl=\"\", string=text)\n",
    "    t2 = re.sub('\\s*\"\\s*', repl=\"\", string=t1)\n",
    "    t3 = re.sub(\"\\s*“\\s*\", repl=\"\", string=t2)\n",
    "    t4 = re.sub(\"\\s*”\\s*\", repl=\"\", string=t3)\n",
    "\n",
    "    return t4\n",
    "\n",
    "def extract_generation_output(response_content):\n",
    "    \"\"\"\n",
    "    Extract the generated fact, source and type of fact from the response content\n",
    "\n",
    "    params:\n",
    "        response_content: the generated content to be extracted\n",
    "    \n",
    "    returns:\n",
    "        generated_fact: the generated fact\n",
    "        source: the source of the fact\n",
    "        type_of_fact: the type of fact\n",
    "    \"\"\"\n",
    "\n",
    "    generated_fact = re.findall(GENERATED_FACT_PATTERN, response_content)[0]\n",
    "    source = re.findall(SOURCE_PATTERN, response_content)[0]\n",
    "    type_of_fact = re.findall(TYPE_OF_FACT_PATTERN, response_content)[0]\n",
    "\n",
    "    return generated_fact, source, type_of_fact\n",
    "\n",
    "\n",
    "def post_process_fact(text):\n",
    "    \"\"\"\n",
    "    Post process the fact to remove unnecessary characters and emojis\n",
    "\n",
    "    params:\n",
    "        text: the text to be cleaned\n",
    "\n",
    "    returns:\n",
    "        cleaned_text: the cleaned text\n",
    "    \"\"\"\n",
    "    t1 = remove_unnecessary_char(text)\n",
    "    t2 = remove_emojis(t1)\n",
    "    generated_fact, source, type_of_fact = extract_generation_output(t2)\n",
    "    return generated_fact, source, type_of_fact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION FUNCTIONS - NOT TO BE DDEPLOYED FOR PRODUCTION\n",
    "\n",
    "def extract_evaluation_results(evaluation_response):\n",
    "    \"\"\"\n",
    "    Extract the evaluation metrics and their results from the evaluation response\n",
    "\n",
    "    params:\n",
    "        evaluation_response: the response from GPT-4 output\n",
    "    \n",
    "    returns:\n",
    "        r1: result_rating\n",
    "        r2: result_relevance\n",
    "        r3: source_credibility\n",
    "        r4: rating_reason\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        r1 = re.search(r\"result_rating=([0-9]+)\", evaluation_response).groups()[0]\n",
    "        r2 = re.search(r\"result_relevance=([0-1])\", evaluation_response).groups()[0]\n",
    "        r3 = re.search(r\"source_credibility=([0-1])\", evaluation_response).groups()[0]\n",
    "        r4 = re.search(r\"rating_reason=(.*)\", evaluation_response).groups()[0]\n",
    "        if r1 and r2 and r3 and r4:\n",
    "            break\n",
    "    return r1, r2, r3, r4\n",
    "\n",
    "\n",
    "def main_evaluate_fact(query, fact):\n",
    "    \"\"\"\n",
    "    Evaluate the generated fact using GPT-4 with the following criteria:\n",
    "        - Is the fact generated related to the query?\n",
    "        - Is the source of the fact credible?\n",
    "    \n",
    "    params:\n",
    "        query: the query to be checked\n",
    "        fact: the generated fact\n",
    "    \n",
    "    returns:\n",
    "        result_rating: the rating of the fact generated\n",
    "        result_relevance: the relevance of the fact generated\n",
    "        source_credibility: the credibility of the source of the fact generated\n",
    "        rating_reason: the reasoning of the rating\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = EVALUATION_PROMPT.format(search_query=query, fact=fact)\n",
    "    response, evaluation_response_content = get_gpt_response(prompt, model=\"gpt-4\")\n",
    "    if evaluation_response_content:\n",
    "        result_rating, result_relevance, source_credibility, rating_reason = extract_evaluation_results(evaluation_response_content)\n",
    "    return result_rating, result_relevance, source_credibility, rating_reason"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single input test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"query\": \"loans\", \"fact\": \"The average American has about $38,000 in personal debt, excluding home mortgages.\", \"source\": \"Northwestern Mutual\", \"fact_type\": \"Statistic\", \"hide_flag\": false}\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Enter query: \")\n",
    "fact,flag = main_generate_fact(query)\n",
    "if fact is not None:\n",
    "    # Evaluation\n",
    "        # result_rating, result_relevance, source_credibility, rating_reason = main_evaluate_fact(query, fact)\n",
    "\n",
    "    if flag == 0:\n",
    "        generated_fact, source, type_of_fact = post_process_fact(fact)\n",
    "    else:\n",
    "        generated_fact = \"Query contains restricted keywords\"\n",
    "        source = None\n",
    "        type_of_fact = None\n",
    "    json_response = {\n",
    "        \"query\": query,\n",
    "        \"fact\": generated_fact, \n",
    "        \"source\": source,\n",
    "        \"fact_type\": type_of_fact,\n",
    "        \"hide_flag\": flag\n",
    "    } \n",
    "    output = json.dumps(json_response)\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scope Test - Run seperately\n",
    "\n",
    "#### Test Dataset used : Google's Top Financial and Economy Search terms in the Past 5 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = pd.read_csv('../../input/search_sample_queries.csv')\n",
    "#only get random 50 queries\n",
    "df_dataset = df_dataset.sample(n=50)\n",
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_dataset.iterrows():\n",
    "    query = row['Top 100 Queries & Suggestions']\n",
    "    category = 'Universal Search Sample'\n",
    "    if query in df_dataset['Query'].values:\n",
    "        continue\n",
    "    fact,flag = main_generate_fact(query)\n",
    "    if fact is not None:\n",
    "        # Evaluation\n",
    "            # result_rating, result_relevance, source_credibility, rating_reason = main_evaluate_fact(query, fact)\n",
    "        if flag == 0:\n",
    "            generated_fact = re.findall(GENERATED_FACT_PATTERN, fact)[0]\n",
    "            source = re.findall(SOURCE_PATTERN, fact)[0]\n",
    "            type_of_fact = re.findall(TYPE_OF_FACT_PATTERN, fact)[0]\n",
    "        else:\n",
    "            generated_fact = \"Query contains restricted keywords\"\n",
    "            source = None\n",
    "            type_of_fact = None\n",
    "        json_response = {\n",
    "            \"query\": query,\n",
    "            \"fact\": generated_fact, \n",
    "            \"source\": source,\n",
    "            \"fact_type\": type_of_fact,\n",
    "            \"hide_flag\": flag\n",
    "        } \n",
    "        output = json.dumps(json_response)\n",
    "        print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the length of query\n",
    "df_dataset['Query_Length'] = df_dataset['Query'].str.len()\n",
    "df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('../../output/financial_economy_query_dataset_output.csv', index=False)\n",
    "df_dataset.to_csv('../../output/moneylion_search_queries_output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gai_srch_summarization_p39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
